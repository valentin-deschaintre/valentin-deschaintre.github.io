papers:

- title: "TileGen: Tileable, Controllable Material Generation and Capture"
  authors: 
    - "Xilong Zhou"
    - "Milos Hasan"
    - "Valentin Deschaintre"
    - "Paul Guerrero"
    - "Kalyan Sunkavalli"
    - "Nima Kalantari"
  abstract: "Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate per-pixel material maps, or as a prior to reconstruct materials from input photographs. These models can generate varied random material appearance, but do not have any mechanism to constrain the generated material to a specific category or to control the coarse structure of the generated material, such as the exact brick layout on a brick wall. Furthermore, materials reconstructed from a single input photo commonly have artifacts and are generally not tileable, which limits their use in practical content creation pipelines. We propose TileGen, a generative model for SVBRDFs that is specific to a material category, always tileable, and optionally conditional on a provided input structure pattern.  TileGen is a variant of StyleGAN whose architecture is modified to always produce tileable (periodic) material maps. In addition to the standard 'style' latent code, TileGen can optionally take a condition image, giving a user direct control over the dominant spatial (and optionally color) features of the material. For example, in brick materials, the user can specify a brick layout and the brick color, or in leather materials, the locations of wrinkles and folds. Our inverse rendering approach can find a material perceptually matching a single target photograph by optimization. This reconstruction can also be conditional on a user-provided pattern. The resulting materials are tileable, can be larger than the target image, and are editable by varying the condition."
  full_teaser: "img/tileGen.jpg"
  square_teaser: "img/tileGen.jpg"
  date: "2022-06"
  paper_link: "https://arxiv.org/pdf/2206.05649"
  website_link: "https://arxiv.org/abs/2206.05649"
  citation: "TileGen: Tileable, Controllable Material Generation and Capture. Xilong Zhou, Miloš Hašan, Valentin Deschaintre, Paul Guerrero, Kalyan Sunkavalli, Nima Kalantari, Arxiv, Jun 2022"


- title: "Node Graph Optimization Using Differentiable Proxies"
  authors: 
    - "Yiwei Hu"
    - "Paul Guerrero"
    - "Milos Hasan"
    - "Holly Rushmeier"
    - "Valentin Deschaintre"
  abstract: "Graph-based procedural materials are ubiquitous in content production industries. Procedural models allow the creation of photorealistic materials with parametric control for flexible editing of appearance. However, designing a specific material is a time-consuming process in terms of building a model and fine-tuning parameters. Previous work [Hu et al. 2022; Shi et al. 2020] introduced material graph optimization frameworks for matching target material samples. However, these previous methods were limited to optimizing differentiable functions in the graphs. In this paper, we propose a fully differentiable framework which enables end-to-end gradient based optimization of material graphs, even if some functions of the graph are non-differentiable. We leverage the Differentiable Proxy, a differentiable approximator of a non-differentiable black-box function. We use our framework to match structure and appearance of an output material to a target material, through a multi-stage differentiable optimization. Differentiable Proxies offer a more general optimization solution to material appearance matching than previous work."
  full_teaser: "img/siggraph22.jpg"
  square_teaser: "img/siggraph22.jpg"
  date: "2022-08"
  paper_link: "https://yiweihu.netlify.app/publication/node-graph-optimization-using-differentiable-proxies/"
  website_link: "https://yiweihu.netlify.app/publication/node-graph-optimization-using-differentiable-proxies/"
  citation: "Node Graph Optimization Using Differentiable Proxies. Yiwei Hu, Paul Guerrero, Miloš Hašan, Holly Rushmeier, Valentin Deschaintre, Siggraph, August 2022"

- title: "Controlling Material Appearance by Examples"
  authors: 
    - "Yiwei Hu"
    - "Milos Hasan"
    - "Paul Guerrero"
    - "Holly Rushmeier"
    - "Valentin Deschaintre"
  abstract: "Despite the ubiquitousness of materials maps in modern rendering pipelines, their editing and control remains a challenge. In this paper, we present an example-based material control method to augment input material maps based on user-provided material photos. We train a tileable version of MaterialGAN and leverage its material prior to guide the appearance transfer, optimizing its latent space using differentiable rendering. Our method transfers the micro and meso-structure textures of user provided target(s) photographs, while preserving the structure of the input and quality of the input material. We show our methods can control existing material maps, increasing realism or generating new, visually appealing materials."
  full_teaser: "img/egsr22.jpg"
  square_teaser: "img/egsr22.jpg"
  date: "2022-07"
  paper_link: "https://graphics.cs.yale.edu/sites/default/files/controlling_material_appearance_by_examples_preprint.pdf"
  website_link: "https://yiweihu.netlify.app/publication/controlling-material-appearance-by-examples/"
  citation: "Controlling Material Appearance by Examples. Yiwei Hu, Miloš Hašan, Paul Guerrero, Holly Rushmeier, Valentin Deschaintre, Computer Graphics Forum (Eurographics Symposium on Rendering Conference Proceedings), July 2022"

- title: "Spectral Upsampling Approaches for RGB Illumination"
  authors: 
    - "Claudio Guarnera"
    - "Yuliya Gitlina"
    - "Valentin Deschaintre"
    - "Abhijeet Ghosh"
  abstract: "We present two practical approaches for high fidelity spectral upsampling of previously recorded RGB illumination in the form of an image-based representation such as an RGB light probe. Unlike previous approaches that require multiple measurements with a spectrometer or a reference color chart under a target illumination environment, our method requires no additional information for the spectral upsampling step. Instead, we construct a data-driven basis of spectral distributions for incident illumination from a set of six RGBW LEDs (three narrowband and three broadband) that we employ to represent a given RGB color using a convex combination of the six basis spectra. We propose two different approaches for estimating the weights of the convex combination using – (a) genetic algorithm, and (b) neural networks. We additionally propose a theoretical basis consisting of a set of narrow and broad Gaussians as a generalization of the approach, and also evaluate an alternate LED basis for spectral upsampling. We achieve good qualitative matches of the predicted illumination spectrum using our spectral upsampling approach to ground truth illumination spectrum while achieving near perfect matching of the RGB color of the given illumination in the vast majority of cases. We demonstrate that the spectrally upsampled RGB illumination can be employed for various applications including improved lighting reproduction as well as more accurate spectral rendering."
  full_teaser: "img/egsr_claudio.jpg"
  square_teaser: "img/egsr_claudio.jpg"
  date: "2022-06"
  paper_link: "https://diglib.eg.org/bitstream/handle/10.2312/sr20221150/001-012.pdf?sequence=1&isAllowed=y"
  website_link: "https://diglib.eg.org/handle/10.2312/sr20221150"
  citation: "Spectral Upsampling Approaches for RGB Illumination. Claudio Giuseppe Guarnera, Yuliya Gitlina, Valentin Deschaintre, Abhijeet Ghosh, Eurographics Symposium on Rendering - Symposium Track, July 2022"


- title: "An Inverse Procedural Modeling Pipeline for SVBRDF Maps"
  authors: 
    - "Yiwei Hu"
    - "Chengan He"
    - "Valentin Deschaintre"
    - "Julie Dorsey"
    - "Holly Rushmeier"
  abstract: "Procedural modeling is now the de facto standard of material modeling in industry. Procedural models can be edited and are easily extended, unlike pixel-based representations of captured materials. In this paper, we present a semi-automatic pipeline for general material proceduralization. Given Spatially-Varying Bidirectional Reflectance Distribution Functions (SVBRDFs) represented as sets of pixel maps, our pipeline decomposes them into a tree of sub-materials whose spatial distributions are encoded by their associated mask maps. This semi-automatic decomposition of material maps progresses hierarchically, driven by our new spectrum-aware material matting and instance-based decomposition methods. Each decomposed sub-material is proceduralized by a novel multi-layer noise model to capture local variations at different scales. Spatial distributions of these sub-materials are modeled either by a by-example inverse synthesis method recovering Point Process Texture Basis Functions (PPTBF) or via random sampling. To reconstruct procedural material maps, we propose a differentiable rendering-based optimization that recomposes all generated procedures together to maximize the similarity between our procedural models and the input material pixel maps. We evaluate our pipeline on a variety of synthetic and real materials. We demonstrate our method's capacity to process a wide range of material types, eliminating the need for artist designed material graphs required in previous work. As fully procedural models, our results expand to arbitrary resolution and enable high level user control of appearance."
  full_teaser: "img/teaser_proc_21.png"
  square_teaser: "img/teaser_proc_21.png"
  date: "2022-01"
  paper_link: "https://arxiv.org/pdf/2109.06395.pdf"
  website_link: "https://yiweihu.netlify.app/publication/an-inverse-procedural-modeling-pipeline-for-svbrdf-maps/"
  citation: "An Inverse Procedural Modeling Pipeline for SVBRDF Maps. Yiwei Hu, Chengan He, Valentin Deschaintre, Julie Dorsey, Holly Rushmeier, ACM Trans Graph, April 2022 (Proc. SIGGRAPH 2022)"


- title: "Generative Modelling of BRDF Textures from Flash Images"
  authors: 
    - "Philipp Henzler"
    - "Valentin Deschaintre"
    - "Niloy Mitra"
    - "Tobias Ritschel"
  abstract: "We present a novel method for efficient acquisition of shape and spatially varying reflectance of 3D objects using polarization cues. Unlike previous works that have exploited polarization to estimate material or object appearance under certain constraints (known shape or multiview acquisition), we lift such restrictions by coupling polarization imaging with deep learning to achieve high quality estimate of 3D object shape (surface normals and depth) and SVBRDF using single-view polarization imaging under frontal flash illumination. In addition to acquired polarization images, we provide our deep network with strong novel cues related to shape and reflectance, in the form of a normalized Stokes map and an estimate of diffuse color. We additionally describe modifications to network architecture and training loss which provide further qualitative improvements. We demonstrate our approach to achieve superior results compared to recent works employing deep learning in conjunction with flash illumination."
  full_teaser: "https://henzler.github.io/publication/neuralmaterial/teaser_hub0e1185201b4ddcc4191db33e82aca2a_3252553_720x0_resize_q90_lanczos.jpg"
  square_teaser: "img/teaser_stoch_brdf.png"
  date: "2021-08"
  paper_link: "https://arxiv.org/pdf/2102.11861.pdf"
  website_link: "https://henzler.github.io/publication/neuralmaterial/"
  citation: "Generative Modelling of BRDF Textures from Flash Images. Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel, ACM Trans Graph (Proc. SIGGRAPH Asia 2021)"

- title: "Deep Polarization Imaging for 3D Shape and SVBRDF Acquisition"
  authors: 
    - "Valentin Deschaintre"
    - "Yiming Lin"
    - "Abhijeet Ghosh"
  abstract: "We present a novel method for efficient acquisition of shape and spatially varying reflectance of 3D objects using polarization cues. Unlike previous works that have exploited polarization to estimate material or object appearance under certain constraints (known shape or multiview acquisition), we lift such restrictions by coupling polarization imaging with deep learning to achieve high quality estimate of 3D object shape (surface normals and depth) and SVBRDF using single-view polarization imaging under frontal flash illumination. In addition to acquired polarization images, we provide our deep network with strong novel cues related to shape and reflectance, in the form of a normalized Stokes map and an estimate of diffuse color. We additionally describe modifications to network architecture and training loss which provide further qualitative improvements. We demonstrate our approach to achieve superior results compared to recent works employing deep learning in conjunction with flash illumination."
  full_teaser: "http://wp.doc.ic.ac.uk/rgi/wp-content/uploads/sites/74/2021/03/teaser1-scaled.jpg"
  square_teaser: "img/teaser4.jpg"
  date: "2021-06"
  paper_link: "http://wp.doc.ic.ac.uk/rgi/wp-content/uploads/sites/74/2021/05/deep_polar_cvpr_author_version.pdf"
  youtube_link: "https://www.youtube.com/watch?v=QUTDlgF5ih0"
  website_link: "https://wp.doc.ic.ac.uk/rgi/project/deep-polarization-3d-imaging/"
  citation: "Deep polarization imaging for 3D shape and SVBRDF acquisition. Valentin Deschaintre, Yiming Lin, and Abhijeet Ghosh. Proc. of CVPR, June 2021. <b>(Patent Pending)</b>"
  miscs:
  - "Our paper was accepted as <strong>CVPR Oral</strong>."


- title: "Guided Fine-Tuning for Large-Scale Material Transfer"
  authors: 
    - "Valentin Deschaintre"
    - "George Drettakis"
    - "Adrien Bousseau"
  abstract: "We present a method to transfer the appearance of one or a few exemplar SVBRDFs to a target image representing similar materials. Our solution is extremely simple: we fine-tune a deep appearance-capture network on the provided exemplars, such that it learns to extract similar SVBRDF values from the target image. We introduce two novel material capture and design workflows that demonstrate the strength of this simple approach. Our first workflow allows to produce plausible SVBRDFs of large-scale objects from only a few pictures. Specifically, users only need take a single picture of a large surface and a few close-up flash pictures of some of its details. We use existing methods to extract SVBRDF parameters from the close-ups, and our method to transfer these parameters to the entire surface, enabling the lightweight capture of surfaces several meters wide such as murals, floors and furniture. In our second workflow, we provide a powerful way for users to create large SVBRDFs from internet pictures by transferring the appearance of existing, pre-designed SVBRDFs. By selecting different exemplars, users can control the materials assigned to the target image, greatly enhancing the creative possibilities offered by deep appearance capture."
  full_teaser: "https://team.inria.fr/graphdeco/files/2020/06/teaser_full.png"
  square_teaser: "img/teaser3.jpg"
  date: "2020-07"
  paper_link: "https://www-sop.inria.fr/reves/Basilic/2020/DDB20/guided_acquisition_svbrdf.pdf"
  presentation_link: "https://drive.google.com/file/d/1t5JHcEjAEA7VDPVGRtcND4ECUEVhlyK0/view?usp=sharing"
  youtube_link: "https://www.youtube.com/watch?v=uzF7zdANOuk&t=42s"
  website_link: "https://team.inria.fr/graphdeco/projects/large-scale-materials/"
  citation: "Computer Graphics Forum (Eurographics Symposium on Rendering Conference Proceedings), Volume 39, Number 4 - jul 2020"
 
 
- title: "Lightweight material acquisition using deep learning"
  authors: 
    - "Valentin Deschaintre"
  abstract: "Whether it is used for entertainment or industrial design, computer graphics is ever more present in our everyday life. Yet, reproducing a real scene appearance in a virtual environment remains a challenging task, requiring long hours from trained artists. A good solution is the acquisition of geometries and materials directly from real world examples, but this often comes at the cost of complex hardware and calibration processes. In this thesis, we focus on lightweight material appearance capture to simplify and accelerate the acquisition process and solve industrial challenges such as result image resolution or calibration. Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in pictures. Designing algorithms able to leverage these cues to recover spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a few images has challenged computer graphics researchers for decades. We explore the use of deep learning to tackle lightweight appearance capture and make sense of these visual cues. Once trained, our networks are capable of recovering per-pixel normals, diffuse albedo, specular albedo and specular roughness from as little as one picture of a flat surface lit by the environment or a hand-held flash. We show how our method improves its prediction with the number of input pictures to reach high quality reconstructions with up to 10 images --- a sweet spot between existing single-image and complex multi-image approaches --- and allows to capture large scale, HD materials. We achieve this goal by introducing several innovations on training data acquisition and network design, bringing clear improvement over the state of the art for lightweight material capture. "
  square_teaser: "img/ucaLogo.jpg"
  date: "2019-11"
  paper_link: "https://hal.archives-ouvertes.fr/tel-02418445/"
  website_link: "https://hal.archives-ouvertes.fr/tel-02418445/"
  citation: "Université Côté d'Azur, Inria, PhD Thesis, Nov 2019"
  miscs:
    - "My thesis was awarded the <a href = 'https://gdr-igrv.icube.unistra.fr/index.php/Prix_de_th%C3%A8se_du_GdR_IG-RV'>French Computer Graphics and Geometry Thesis award 2020</a>"
    - "My thesis was awarded the <a href = 'https://fondation-uca.org/projets/bourses-excellence-academique-theses/'>UCA foundation Academic excellence thesis award 2020</a>"
    - "A 5 minutes video describing my work and contributions is available here: <a href = 'https://www.youtube.com/watch?v=uNVxFNJrfEc&feature=youtu.be'>[english]</a><a href = 'https://www.youtube.com/watch?v=SEMtruiPirM&feature=youtu.be'>[french]</a>"

- title: "Material acquisition using deep learning"
  authors: 
    - "Valentin Deschaintre"
  abstract: " Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in pictures. Designing algorithms able to leverage these cues to recover spatiallyvarying bi-directional reflectance distribution functions (SVBRDFs) from a few images has challenged computer graphics researchers for decades. I explore the use of deep learning to tackle lightweight appearance capture and make sense of these visual cues. Our networks are capable of recovering per-pixel normals, diffuse albedo, specular albedo and specular roughness from as little as one picture of a flat surface lit by a hand-held flash. We propose a method which improves its prediction with the number of input pictures, and reaches high quality reconstructions with up to 10 images â€“ a sweet spot between existing single-image and complex multi-image approaches. We introduce several innovations on training data acquisition and network design, bringing clear improvement over the state of the art for lightweight material capture. "
  square_teaser: "files/sigasia/resultOnlyMatrixLR.jpg"
  date: "2019-11"
  paper_link: "https://www-sop.inria.fr/reves/Basilic/2019/Des19/doctoralConsortium_Deschaintre.pdf"
  presentation_link: "https://repo-sam.inria.fr/fungraph/deep_mat_doc_consortium/19_11_17_sigAsia_DocConsortium.pptx"
  poster_link: "https://www-sop.inria.fr/reves/Basilic/2019/Des19/poster_sigAsia.pdf"
  website_link: "https://www-sop.inria.fr/reves/Basilic/2019/Des19/"
  citation: "Asia, Doctoral Consortium (Poster and Short Paper), Nov 2019"
  
- title: "Flexible SVBRDF Capture with a Multi-Image Deep Network"
  authors: 
    - "Valentin Deschaintre"
    - "Miika Aittala"
    - "Fredo Durand"
    - "George Drettakis"
    - "Adrien Bousseau"
  abstract: " Empowered by deep learning, recent methods for material capture can estimate a spatially-varying reflectance from a single photograph. Such lightweight capture is in stark contrast with the tens or hundreds of pictures required by traditional optimization-based approaches. However, a single image is often simply not enough to observe the rich appearance of real-world materials. We present a deep-learning method capable of estimating material appearance from a variable number of uncalibrated and unordered pictures captured with a handheld camera and flash. Thanks to an order-independent fusing layer, this architecture extracts the most useful information from each picture, while benefiting from strong priors learned from data. The method can handle both view and light direction variation without calibration. We show how our method improves its prediction with the number of input pictures, and reaches high quality reconstructions with as little as 1 to 10 images -- a sweet spot between existing single-image and complex multi-image approaches "
  full_teaser: "http://www-sop.inria.fr/reves/Basilic/2019/DADDB19/teaser_full.jpg"
  square_teaser: "img/teaser2.jpg"
  date: "2019-07"
  paper_link: "https://www-sop.inria.fr/reves/Basilic/2019/DADDB19/Flexible_multi_inputs_svbrdf.pdf"
  website_link: "https://team.inria.fr/graphdeco/projects/multi-materials/"
  presentation_link: "https://repo-sam.inria.fr/fungraph/multi_image_materials/EGSR2019_Multi_Image_Final.pptx"
  citation: "Computer Graphics Forum (Eurographics Symposium on Rendering Conference Proceedings), Volume 38, Number 4, page 13 - jul 2019"
  
- title: "Single-Image SVBRDF Capture with a Rendering-Aware Deep Network"
  authors: 
    - "Valentin Deschaintre"
    - "Miika Aittala"
    - "Fredo Durand"
    - "George Drettakis"
    - "Adrien Bousseau"
  abstract: "Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades.
We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design.
For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation.
Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs."
  full_teaser: "https://team.inria.fr/graphdeco/files/2018/08/teaser_v0.png"
  square_teaser: "https://valentin.deschaintre.fr/img/teaser.jpg"
  date: "2018-07"
  paper_link: "https://www-sop.inria.fr/reves/Basilic/2018/DADDB18/Deep%20Material%20Acquisition%20Authors_version.pdf"
  presentation_link: "https://repo-sam.inria.fr/fungraph/deep-materials/18_08_15_siggraph_Deep_Materials_final.pptx"
  website_link: "https://team.inria.fr/graphdeco/projects/deep-materials/"
  citation: "ACM Transactions on Graphics (SIGGRAPH Conference Proceedings), Volume 37, Number 128, page 15 - aug 2018"
  miscs:
    - "<a href = 'https://www.youtube.com/watch?v=UkWnExEFADI'>2 Minutes paper video</a>"
 
